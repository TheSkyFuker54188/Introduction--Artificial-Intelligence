# 《Matching papers and reviewers at large conferences》简要概览

## 论文核心问题
- 大型 AI 会议投稿暴增，传统人工指派评审效率低、质量难控。
- 现有自动化方案易受数据缺失、投标操纵、合作者冲突等问题影响。
- 需要一个既能扩展到上万篇投稿、又能抵御恶意行为、兼顾公平性的匹配系统。

## LCM 系统框架
1. **数据与评分**
   - 汇聚 TPMS、ACL、关键词匹配（SAM）与投标，归一化成 aggscore。
   - 自动识别/补充冲突（合著、导师关系）并过滤可疑投标。
2. **混合整数规划（MIP）指派**
   - 硬约束：冲突剔除、每篇论文/评审的数量限制。
   - 软约束：至少一位资深评审、减少合作者、提升地区多样性、阻断互投互评。
   - 列/行生成让问题规模可控，设置 MIP gap 容忍度加速求解。
3. **双阶段评审流程**
   - 阶段 1：每篇先分两位评审，若高置信度一致拒稿直接出局。
   - 阶段 2：剩余论文补评审、讨论与申辩，把资源集中在有争议论文上。

## 实验亮点与结论
- 基于 AAAI 2021 数据（6729 篇、8072 名评审）验证。
- LCM 大幅降低软约束违例（合作者配对降 80%+）且目标值仅损失约 2%。
- 双阶段流程节省 2615 份评审任务，估计误拒率仅 2.9%，额外评审显著降低评分方差。
- 系统表现促成多届 AAAI、IJCAI、ICML 等会议采用。

## 一句话总结
LCM 通过精细的评分融合、MIP 优化和双阶段流程，为超大规模会议提供兼顾质量、公平与鲁棒性的评审匹配解决方案。